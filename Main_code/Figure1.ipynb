{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate causal maps with reanalysis data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "import matplotlib.patches as patches\n",
    "import cartopy.feature\n",
    "import statsmodels.api as sm\n",
    "from scipy import signal\n",
    "from scipy.signal import detrend\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions and class\n",
    "\n",
    "class JetAnalysis:\n",
    "    def __init__(self, jet_data, covariates_dict, lon1=140, lon2=295):\n",
    "        \"\"\"\n",
    "        Initialize the class with the jet data and a dictionary of covariates.\n",
    "        \n",
    "        Parameters:\n",
    "        - jet_data: xarray.DataArray\n",
    "            The jet data with dimensions lat, lon, and time.\n",
    "        - covariates_dict: dict\n",
    "            Dictionary of covariates, where each key is a predictor name and each value is an xarray.DataArray.\n",
    "        - lon1: int\n",
    "            The starting longitude for the analysis (default is 140).\n",
    "        - lon2: int\n",
    "            The ending longitude for the analysis (default is 295).\n",
    "        \"\"\"\n",
    "        self.jet_data = jet_data\n",
    "        self.covariates_dict = covariates_dict\n",
    "        self.lon1 = lon1\n",
    "        self.lon2 = lon2\n",
    "        self.jet_lat = None\n",
    "        self.jet_strength = None\n",
    "        self.regression_results = None\n",
    "    \n",
    "    def jet_lat_strength(self):\n",
    "        \"\"\"\n",
    "        Calculate the jet latitude and jet strength from the input jet data.\n",
    "        \n",
    "        Returns:\n",
    "        - jet_lat: np.array\n",
    "            Array of calculated jet latitudes over time.\n",
    "        - jet_strength: np.array\n",
    "            Array of calculated jet strengths over time.\n",
    "        \"\"\"\n",
    "        jet_30_70 = self.jet_data.sel(lat=slice(-70, -30)).sel(lon=slice(self.lon1, self.lon2)).mean(dim='lon')\n",
    "        lat = jet_30_70.lat\n",
    "        jet_lat = (jet_30_70 * lat).sum(dim='lat') / jet_30_70.sum(dim='lat')\n",
    "        \n",
    "        strength = []\n",
    "        for t, max_lat in zip(self.jet_data.time, jet_lat):\n",
    "            strength.append(self.jet_data.sel(time=t).sel(lat=max_lat, method='nearest').sel(lon=slice(self.lon1, self.lon2)).mean(dim='lon'))\n",
    "        jet_strength = np.array(strength)\n",
    "        \n",
    "        # Store the results as class attributes\n",
    "        self.jet_lat = np.array(jet_lat.values)\n",
    "        self.jet_strength = jet_strength\n",
    "        \n",
    "        return self.jet_lat, self.jet_strength\n",
    "    \n",
    "    def standardize_data(self, da):\n",
    "        \"\"\"\n",
    "        Standardize an xarray DataArray by subtracting the mean and dividing by the standard deviation.\n",
    "        \n",
    "        Parameters:\n",
    "        - da: xarray.DataArray\n",
    "            Input data to standardize.\n",
    "        \n",
    "        Returns:\n",
    "        - standardized_da: xarray.DataArray\n",
    "            Standardized data.\n",
    "        \"\"\"\n",
    "        mean = da.mean(dim='time')\n",
    "        std_dev = da.std(dim='time')\n",
    "        standardized_da = (da - mean) / std_dev\n",
    "        return standardized_da\n",
    "    \n",
    "    def multiple_linear_regression(self, target, predictors_dict):\n",
    "        \"\"\"\n",
    "        Perform multiple linear regression on a target time series using a dictionary of predictors.\n",
    "        \n",
    "        Parameters:\n",
    "        - target: xarray.DataArray\n",
    "            The target time series to predict.\n",
    "        - predictors_dict: dict\n",
    "            A dictionary where keys are predictor names and values are xarray.DataArray objects representing predictor time series.\n",
    "        \n",
    "        Returns:\n",
    "        - results: statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "            The results of the regression, including coefficients, p-values, etc.\n",
    "        \"\"\"\n",
    "        # Convert the target and predictors to a pandas DataFrame\n",
    "        df = pd.DataFrame({name: da.to_series() for name, da in predictors_dict.items()})\n",
    "        \n",
    "        # Ensure the target time series is aligned with predictors\n",
    "        df['target'] = target.to_series()\n",
    "        \n",
    "        # Drop any rows with NaN values\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Separate the predictors and the target\n",
    "        X = df.drop(columns='target')\n",
    "        y = df['target']\n",
    "\n",
    "        # Add a constant (intercept) to the predictors\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # Perform the OLS regression\n",
    "        model = sm.OLS(y, X)\n",
    "        results = model.fit()\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze(self):\n",
    "        \"\"\"\n",
    "        Perform the full analysis: calculate metrics, standardize data, perform regression, and save results.\n",
    "        \n",
    "        Returns:\n",
    "        - results_dict: dict\n",
    "            A dictionary containing the regression coefficients, p-values, and summary.\n",
    "        \"\"\"\n",
    "        # Step 1: Calculate the jet latitude and strength\n",
    "        jet_lat, jet_strength = self.jet_lat_strength()\n",
    "\n",
    "        # Step 2: Standardize the data\n",
    "        standardized_jet_lat = self.standardize_data(xr.DataArray(jet_lat, dims=['time'], coords={'time': self.jet_data.time}))\n",
    "        standardized_jet_strength = self.standardize_data(xr.DataArray(jet_strength, dims=['time'], coords={'time': self.jet_data.time}))\n",
    "\n",
    "        # Standardize the predictors\n",
    "        standardized_predictors = {name: self.standardize_data(da) for name, da in self.covariates_dict.items()}\n",
    "\n",
    "        # Step 3: Perform multiple linear regression on both metrics\n",
    "        lat_results = self.multiple_linear_regression(standardized_jet_lat, standardized_predictors)\n",
    "        strength_results = self.multiple_linear_regression(standardized_jet_strength, standardized_predictors)\n",
    "\n",
    "        # Save the regression results\n",
    "        self.regression_results = {\n",
    "            'jet_lat_regression': lat_results,\n",
    "            'jet_strength_regression': strength_results\n",
    "        }\n",
    "\n",
    "        # Step 4: Compile the results into a dictionary\n",
    "        results_dict = {\n",
    "            'jet_lat_coefficients': lat_results.params.to_dict(),\n",
    "            'jet_lat_pvalues': lat_results.pvalues.to_dict(),\n",
    "            'jet_lat_summary': lat_results.summary().as_text(),\n",
    "            'jet_strength_coefficients': strength_results.params.to_dict(),\n",
    "            'jet_strength_pvalues': strength_results.pvalues.to_dict(),\n",
    "            'jet_strength_summary': strength_results.summary().as_text()\n",
    "        }\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "\n",
    "def detrend_timeseries(da):\n",
    "    \"\"\"\n",
    "    Remove the linear trend from an xarray DataArray.\n",
    "\n",
    "    Parameters:\n",
    "    - da: xarray.DataArray\n",
    "        The input time series to detrend.\n",
    "\n",
    "    Returns:\n",
    "    - detrended_da: xarray.DataArray\n",
    "        The detrended time series.\n",
    "    \"\"\"\n",
    "    # Check if the input is a DataArray\n",
    "    if not isinstance(da, xr.DataArray):\n",
    "        raise TypeError(\"Input must be an xarray DataArray\")\n",
    "\n",
    "    # Ensure that there is a 'time' coordinate\n",
    "    if 'time' not in da.coords:\n",
    "        raise ValueError(\"DataArray must have a 'time' coordinate\")\n",
    "\n",
    "    # Detrend the data along the time axis\n",
    "    detrended_data = detrend(da, axis=0)\n",
    "\n",
    "    # Return as a new DataArray, preserving the original coordinates\n",
    "    detrended_da = xr.DataArray(detrended_data, dims=da.dims, coords=da.coords)\n",
    "    \n",
    "    return detrended_da\n",
    "\n",
    "\n",
    "def multiple_linear_regression(target, predictors_dict):\n",
    "    \"\"\"\n",
    "    Perform a multiple linear regression on a target time series using a dictionary of predictor time series.\n",
    "\n",
    "    Parameters:\n",
    "    - target: xarray.DataArray\n",
    "        The target time series to predict.\n",
    "    - predictors_dict: dict\n",
    "        A dictionary where keys are predictor names and values are xarray.DataArray objects representing predictor time series.\n",
    "\n",
    "    Returns:\n",
    "    - results: statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "        The results of the regression, including coefficients, p-values, etc.\n",
    "    \"\"\"\n",
    "    # Check if input is a DataArray\n",
    "    if not isinstance(target, xr.DataArray):\n",
    "        raise TypeError(\"Target must be an xarray DataArray\")\n",
    "    \n",
    "    # Ensure that there is a 'time' coordinate\n",
    "    if 'time' not in target.coords:\n",
    "        raise ValueError(\"Target DataArray must have a 'time' coordinate\")\n",
    "\n",
    "    # Convert the target and predictors to a pandas DataFrame\n",
    "    df = pd.DataFrame({name: da.to_series() for name, da in predictors_dict.items()})\n",
    "    \n",
    "    # Ensure the target time series is aligned with predictors\n",
    "    df['target'] = target.to_series()\n",
    "    \n",
    "    # Drop any rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Separate the predictors and the target\n",
    "    X = df.drop(columns='target')\n",
    "    y = df['target']\n",
    "\n",
    "    # Add a constant (intercept) to the predictors\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Perform the OLS regression\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def standardize_data(da):\n",
    "    \"\"\"\n",
    "    Standardize an xarray DataArray by subtracting the mean and dividing by the standard deviation.\n",
    "    \n",
    "    Parameters:\n",
    "    da (xarray.DataArray or xarray.Dataset): Input data to standardize.\n",
    "    \n",
    "    Returns:\n",
    "    xarray.DataArray or xarray.Dataset: Standardized data.\n",
    "    \"\"\"\n",
    "    mean = da.mean(dim='time')\n",
    "    std_dev = da.std(dim='time')\n",
    "    \n",
    "    standardized_da = (da - mean) / std_dev\n",
    "    return standardized_da\n",
    "\n",
    "\n",
    "def stand_detr(dato):\n",
    "    anom = (dato - np.mean(dato))/np.std(dato)\n",
    "    return signal.detrend(anom)\n",
    "\n",
    "def filtro(dato):\n",
    "    \"\"\"Apply a rolling mean of 5 years and remov the NaNs resulting bigining and end\"\"\"\n",
    "    signal = dato - dato.rolling(time=10, center=True).mean()\n",
    "    signal_out = signal.dropna('time', how='all')\n",
    "    return signal_out\n",
    "                          \n",
    "def stand(dato):\n",
    "    anom = (dato - np.mean(dato))/np.std(dato)\n",
    "    return anom\n",
    "\n",
    "def replace_nans_with_zero(x):\n",
    "    return np.where(np.isnan(x), random.random(), x)\n",
    "\n",
    "def figure(target,predictors):\n",
    "    fig = plt.figure()\n",
    "    y = predictors.apply(stand_detr,axis=0).values\n",
    "    for i in range(len(predictors.keys())):\n",
    "        plt.plot(y[:,i])\n",
    "    plt.plot(stand_detr(target))\n",
    "    return fig\n",
    "\n",
    "def jet_lat_strength(jet_data,lon1=140,lon2=295):\n",
    "    jet_30_70 = jet_data.sel(lat=slice(-30,-70)).sel(lon=slice(lon1,lon2)).mean(dim='lon')\n",
    "    lat = jet_30_70.lat\n",
    "    jet_lat = (jet_30_70*lat).sum(dim='lat')/(jet_30_70).sum(dim='lat')\n",
    "    strength = []\n",
    "    for t,max_lat in zip(jet_data.time,jet_lat):\n",
    "        strength.append(jet_data.sel(time=t).sel(lat=max_lat,method='nearest').sel(lon=slice(lon1,lon2)).mean(dim='lon'))\n",
    "    jet_strength = np.array(strength)\n",
    "    return np.array(jet_lat.values),jet_strength\n",
    "\n",
    "def jet_lat_strength_model(jet_data,lon1=140,lon2=295):\n",
    "    jet_30_70 = jet_data.sel(lat=slice(-70,-30)).sel(lon=slice(lon1,lon2)).mean(dim='lon')\n",
    "    lat = jet_30_70.lat\n",
    "    jet_lat = (jet_30_70*lat).sum(dim='lat')/(jet_30_70).sum(dim='lat')\n",
    "    strength = []\n",
    "    for t,max_lat in zip(jet_data.time,jet_lat):\n",
    "        strength.append(jet_data.sel(time=t).sel(lat=max_lat,method='nearest').sel(lon=slice(lon1,lon2)).mean(dim='lon'))\n",
    "    jet_strength = np.array(strength)\n",
    "    return np.array(jet_lat.values),jet_strength\n",
    "\n",
    "def seasonal_data(data,season='DJF'):\n",
    "    # select DJF\n",
    "    DA_DJF = data.sel(time = data.time.dt.season==season)\n",
    "\n",
    "    # calculate mean per year\n",
    "    DA_DJF = DA_DJF.groupby(DA_DJF.time.dt.year).mean(\"time\")\n",
    "    DA_DJF = DA_DJF.rename({'year':'time'})\n",
    "    return DA_DJF\n",
    "\n",
    "def seasonal_data_months(data, months):\n",
    "    \"\"\"\n",
    "    Selects specified months from an xarray object and averages the data for those months within each year.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: xarray.DataArray or xarray.Dataset\n",
    "        The input data to process. It should have a 'time' coordinate.\n",
    "    - months: list of int\n",
    "        The months to select for averaging (1 = January, 2 = February, ..., 12 = December).\n",
    "    \n",
    "    Returns:\n",
    "    - xarray.DataArray or xarray.Dataset\n",
    "        The averaged data for the selected months within each year, accounting for months that span across years.\n",
    "    \"\"\"\n",
    "    # Ensure 'time' coordinate is in a format that supports .dt accessor\n",
    "    if np.issubdtype(data['time'].dtype, np.datetime64):\n",
    "        time_coord = data['time']\n",
    "    else:\n",
    "        time_coord = xr.cftime_range(start=data['time'][0].values, periods=data['time'].size, freq='M')\n",
    "        data = data.assign_coords(time=time_coord)\n",
    "\n",
    "    # Select the relevant months and keep track of the original years\n",
    "    selected_months_data = data.sel(time=data['time'].dt.month.isin(months))\n",
    "\n",
    "    # Create a new time coordinate for grouping\n",
    "    new_years = selected_months_data['time'].dt.year.values.copy()\n",
    "\n",
    "    # Shift the year for December, if necessary\n",
    "    if 12 in months:\n",
    "        dec_mask = selected_months_data['time'].dt.month == 12\n",
    "        new_years[dec_mask] += 1  # Increment year for December\n",
    "\n",
    "    # Assign the new year as a coordinate to the selected data\n",
    "    selected_months_data = selected_months_data.assign_coords(new_year=(\"time\", new_years))\n",
    "\n",
    "    # Now group by the new year and calculate the mean\n",
    "    averaged_data = selected_months_data.groupby(\"new_year\").mean(dim=\"time\")\n",
    "\n",
    "    # Rename the new year dimension to 'time' for consistency\n",
    "    averaged_data = averaged_data.rename({\"new_year\": \"time\"})\n",
    "\n",
    "    return averaged_data\n",
    "\n",
    "\n",
    "#Across models regression class\n",
    "class spatial_MLR(object):\n",
    "    def __init__(self):\n",
    "        self.what_is_this = 'This performs a regression across models and plots everything'\n",
    "    \n",
    "    def regression_data(self,variable,regressors,regressor_names,dataset):\n",
    "        \"\"\"Define the regression target variable \n",
    "        this is here to be edited if some opperation is needed on the DataArray\n",
    "        \n",
    "        :param variable: DataArray\n",
    "        :return: target variable for the regression  \n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.target = variable\n",
    "        regressor_indices = regressors\n",
    "        self.regression_y = sm.add_constant(regressors.values)\n",
    "        self.regressors = regressors.values\n",
    "        self.rd_num = len(regressor_names) \n",
    "        self.regressor_names = regressor_names\n",
    "\n",
    "    #Regresion lineal\n",
    "    def linear_regression(self,x):\n",
    "        y = self.regression_y\n",
    "        res = sm.OLS(x,y).fit()\n",
    "        returns = [res.params[i] for i in range(self.rd_num)]\n",
    "        return tuple(returns)\n",
    "\n",
    "    def linear_regression_pvalues(self,x):\n",
    "        y = self.regression_y\n",
    "        res = sm.OLS(x,y).fit()\n",
    "        returns = [res.pvalues[i] for i in range(self.rd_num)]\n",
    "        return tuple(returns)\n",
    "    \n",
    "    def linear_regression_R2(self,x):\n",
    "        y = self.regression_y\n",
    "        res = sm.OLS(x,y).fit()\n",
    "        return res.rsquared\n",
    "    \n",
    "\n",
    "    def perform_regression(self,path,var): \n",
    "        \"\"\" Performs regression over all gridpoints in a map and returns and saves DataFrames\n",
    "        \n",
    "        :param path: saving path\n",
    "        :return: none\n",
    "        \"\"\"\n",
    "        \n",
    "        target_var = xr.apply_ufunc(replace_nans_with_zero, self.target)\n",
    "        results = xr.apply_ufunc(self.linear_regression,target_var,input_core_dims=[[\"time\"]],\n",
    "                                 output_core_dims=[[] for i in range(self.rd_num)],\n",
    "                                 vectorize=True,\n",
    "                                 dask=\"parallelized\")\n",
    "        results_pvalues = xr.apply_ufunc(self.linear_regression_pvalues,target_var,input_core_dims=[[\"time\"]],\n",
    "                                 output_core_dims=[[] for i in range(self.rd_num)],\n",
    "                                 vectorize=True,\n",
    "                                 dask=\"parallelized\")\n",
    "        results_R2 = xr.apply_ufunc(self.linear_regression_R2,target_var,input_core_dims=[[\"time\"]],\n",
    "                                 output_core_dims=[[]],\n",
    "                                 vectorize=True,\n",
    "                                 dask=\"parallelized\")\n",
    "        \n",
    "      \n",
    "        for i in range(self.rd_num):\n",
    "            if i == 0:\n",
    "                regression_coefs = results[0].to_dataset(name='const')\n",
    "            else:\n",
    "                regression_coefs[self.regressor_names[i]] = results[i]\n",
    "                \n",
    "        print('This is regressor_coefs:',regression_coefs)\n",
    "        if var == 'ua':\n",
    "            regression_coefs = regression_coefs.rename({'ua':self.regressor_names[0]})\n",
    "        elif var == 'sst':\n",
    "            regression_coefs = regression_coefs.rename({'tos':self.regressor_names[0]})\n",
    "        elif var == 'tas':\n",
    "            regression_coefs = regression_coefs.rename({'tas':self.regressor_names[0]})\n",
    "        elif var == 'pr':\n",
    "            regression_coefs = regression_coefs.rename({'pr':self.regressor_names[0]})\n",
    "        else:\n",
    "            'done'\n",
    "            #regression_coefs = regression_coefs.rename({var:self.regressor_names[0]})\n",
    "        regression_coefs.to_netcdf(path+'/'+var+'/regression_coefficients_'+self.dataset+'.nc')\n",
    "        \n",
    "        for i in range(self.rd_num):\n",
    "            if i == 0:\n",
    "                regression_coefs_pvalues = results_pvalues[0].to_dataset(name='const')\n",
    "            else:\n",
    "                regression_coefs_pvalues[self.regressor_names[i]] = results_pvalues[i]        \n",
    "        if var == 'ua':\n",
    "            regression_coefs_pvalues = regression_coefs_pvalues.rename({'ua':self.regressor_names[0]})\n",
    "        elif var == 'sst':\n",
    "            regression_coefs_pvalues = regression_coefs_pvalues.rename({'tos':self.regressor_names[0]})\n",
    "        elif var == 'tas':\n",
    "            regression_coefs_pvalues = regression_coefs_pvalues.rename({'tas':self.regressor_names[0]})\n",
    "        elif var == 'pr':\n",
    "            regression_coefs_pvalues = regression_coefs_pvalues.rename({'pr':self.regressor_names[0]})\n",
    "        else:\n",
    "            'done'\n",
    "            #regression_coefs_pvalues = regression_coefs_pvalues.rename({var:self.regressor_names[0]})\n",
    "        regression_coefs_pvalues.to_netcdf(path+'/'+var+'/regression_coefficients_pvalues_'+self.dataset+'.nc')\n",
    "        \n",
    "\n",
    "        results_R2.to_netcdf(path+'/'+var+'/R2_'+self.dataset+'.nc')\n",
    "                     \n",
    "        \n",
    "    def create_x(self,i,j,dato):\n",
    "        \"\"\" For each gridpoint creates an array and standardizes it \n",
    "        :param regressor_names: list with strings naming the independent variables\n",
    "        :param path: saving path\n",
    "        :return: none\n",
    "        \"\"\"    \n",
    "        x = np.array([])\n",
    "        for y in range(len(dato.time)):\n",
    "            aux = dato.isel(time=y)\n",
    "            x = np.append(x,aux[i-1,j-1].values)\n",
    "        return stand(x)\n",
    "     \n",
    "    \n",
    "    def open_regression_coef(self,path,var,dataset):\n",
    "        \"\"\" Open regression coefficients and pvalues to plot\n",
    "        :param path: saving path\n",
    "        :return maps: list of list of coefficient maps\n",
    "        :return maps_pval:  list of coefficient pvalues maps\n",
    "        :return R2: map of fraction of variance\n",
    "        \"\"\" \n",
    "        maps = []; maps_pval = []\n",
    "        coef_maps = xr.open_dataset(path+'/'+var+'/regression_coefficients_'+dataset+'.nc')\n",
    "        coef_pvalues = xr.open_dataset(path+'/'+var+'/regression_coefficients_pvalues_'+dataset+'.nc')\n",
    "        maps = [coef_maps[variable] for variable in self.regressor_names]\n",
    "        maps_pval = [coef_pvalues[variable] for variable in self.regressor_names]\n",
    "        R2 = xr.open_dataset(path+'/'+var+'/R2_'+dataset+'.nc')\n",
    "        return maps, maps_pval, R2    \n",
    "\n",
    "    def open_lmg_coef(self,path,var):\n",
    "        \"\"\" Open regression coefficients and pvalues to plot\n",
    "        :param path: saving path\n",
    "        :return maps: list of list of coefficient maps\n",
    "        :return maps_pval:  list of coefficient pvalues maps\n",
    "        :return R2: map of fraction of variance\n",
    "        \"\"\" \n",
    "        maps = []; maps_pval = []\n",
    "        coef_maps = xr.open_dataset(path+'/'+var+'/regression_coefficients_relative_importance.nc')\n",
    "        coef_pvalues = xr.open_dataset(path+'/'+var+'/regression_coefficients_pvalues.nc')\n",
    "        maps = [coef_maps[variable] for variable in self.regressor_names[1:]]\n",
    "        maps_pval = [coef_pvalues[variable] for variable in self.regressor_names]\n",
    "        R2 = xr.open_dataset(path+'/'+var+'/R2.nc')\n",
    "        return maps, maps_pval, R2    \n",
    "    \n",
    "    def plot_regression_lmg_map(self,path,var,output_path):\n",
    "        \"\"\" Plots figure with all of \n",
    "        :param regressor_names: list with strings naming the independent variables\n",
    "        :param path: saving path\n",
    "        :return: none\n",
    "        \"\"\"\n",
    "        maps, maps_pval, R2 = self.open_lmg_coef(path,var)\n",
    "        cmapU850 = mpl.colors.ListedColormap(['darkblue','navy','steelblue','lightblue',\n",
    "                                            'lightsteelblue','white','white','mistyrose',\n",
    "                                            'lightcoral','indianred','brown','firebrick'])\n",
    "        cmapU850.set_over('maroon')\n",
    "        cmapU850.set_under('midnightblue')\n",
    "        path_era = '/datos/ERA5/mon'\n",
    "        u_ERA = xr.open_dataset(path_era+'/era5.mon.mean.nc')\n",
    "        u_ERA = u_ERA.u.sel(lev=850).sel(time=slice('1979','2018'))\n",
    "        u_ERA = u_ERA.groupby('time.season').mean(dim='time').sel(season='DJF')\n",
    "\n",
    "        fig_coef = plt.figure(figsize=(20, 16),dpi=100,constrained_layout=True)\n",
    "        projection_stereo = ccrs.SouthPolarStereo(central_longitude=300)\n",
    "        projection_plate = ccrs.PlateCarree(180)\n",
    "        data_crs = ccrs.PlateCarree()\n",
    "        for k in range(self.rd_num-1):\n",
    "            lat = maps[k].lat\n",
    "            lon = np.linspace(0,360,len(maps[k].lon))\n",
    "            var_c, lon_c = add_cyclic_point(maps[k].values,lon)\n",
    "            #SoutherHemisphere Stereographic\n",
    "            if var == 'ua':\n",
    "                ax = plt.subplot(3,3,k+1,projection=projection_stereo)\n",
    "                ax.set_extent([0,359.9, -90, 0], crs=data_crs)\n",
    "                theta = np.linspace(0, 2*np.pi, 100)\n",
    "                center, radius = [0.5, 0.5], 0.5\n",
    "                verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n",
    "                circle = mpath.Path(verts * radius + center)\n",
    "                ax.set_boundary(circle, transform=ax.transAxes)\n",
    "            elif var == 'sst':\n",
    "                ax = plt.subplot(3,3,k+1,projection=projection_plate)\n",
    "            else: \n",
    "                ax = plt.subplot(3,3,k+1,projection=projection_stereo)\n",
    "            clevels = np.arange(0,40,2)\n",
    "            im=ax.contourf(lon_c, lat, var_c*100,clevels,transform=data_crs,cmap='OrRd',extend='both')\n",
    "            cnt=ax.contour(u_ERA.lon,u_ERA.lat, u_ERA.values,levels=[8],transform=data_crs,linewidths=1.2, colors='black', linestyles='-')\n",
    "            plt.clabel(cnt,inline=True,fmt='%1.0f',fontsize=8)\n",
    "            if maps_pval[k+1].min() < 0.05: \n",
    "                levels = [maps_pval[k+1].min(),0.05,maps_pval[k+1].max()]\n",
    "                ax.contourf(maps_pval[k+1].lon, lat, maps_pval[k+1].values,levels, transform=data_crs,levels=levels, hatches=[\"...\", \" \"], alpha=0)\n",
    "            elif maps_pval[k+1].min() < 0.10:\n",
    "                levels = [maps_pval[k+1].min(),0.10,maps_pval[k+1].max()]\n",
    "                ax.contourf(maps_pval[k+1].lon, lat, maps_pval[k+1].values,levels, transform=data_crs,levels=levels, hatches=[\"...\", \" \"], alpha=0)\n",
    "            else:\n",
    "                print('No significant values for ',self.regressor_names[k+1]) \n",
    "            plt.title(self.regressor_names[k+1],fontsize=18)\n",
    "            ax.add_feature(cartopy.feature.COASTLINE,alpha=.5)\n",
    "            ax.add_feature(cartopy.feature.BORDERS, linestyle='-', alpha=.5)\n",
    "            ax.gridlines(crs=data_crs, linewidth=0.3, linestyle='-')\n",
    "            ax.set_extent([-180, 180, -90, -25], ccrs.PlateCarree())\n",
    "        plt1_ax = plt.gca()\n",
    "        left, bottom, width, height = plt1_ax.get_position().bounds\n",
    "        if var == 'ua':\n",
    "            colorbar_axes1 = fig_coef.add_axes([left+0.5, bottom, 0.01, height*2])\n",
    "        elif var == 'sst':\n",
    "            colorbar_axes1 = fig_coef.add_axes([left+0.3, bottom, 0.01, height*2])    \n",
    "        else:\n",
    "            colorbar_axes1 = fig_coef.add_axes([left+0.5, bottom, 0.01, height*2])\n",
    "        cbar = fig_coef.colorbar(im, colorbar_axes1, orientation='vertical')\n",
    "        cbar.set_label('relative importance',fontsize=14) #rotation = radianes\n",
    "        cbar.ax.tick_params(axis='both',labelsize=14)\n",
    "            \n",
    "        plt.subplots_adjust(bottom=0.2, right=.95, top=0.8)\n",
    "        if var == 'ua':\n",
    "            plt.savefig(output_path+'/regression_coefficients_relative_importance_u850',bbox_inches='tight')\n",
    "        elif var == 'sst':\n",
    "            plt.savefig(output_path+'/regression_coefficients_relative_importance_sst',bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(output_path+'/regression_coefficients_relative_importance_XXX',bbox_inches='tight')   \n",
    "        plt.clf\n",
    "\n",
    "        return fig_coef\n",
    "\n",
    "\n",
    "    def plot_regression_coef_map(self,path,var,output_path):\n",
    "        \"\"\" Plots figure with all of \n",
    "        :param regressor_names: list with strings naming the independent variables\n",
    "        :param path: saving path\n",
    "        :return: none\n",
    "        \"\"\"\n",
    "        maps, maps_pval, R2 = self.open_regression_coef(path,var,self.dataset)\n",
    "        cmapU850 = mpl.colors.ListedColormap(['darkblue','navy','steelblue','lightblue',\n",
    "                                            'lightsteelblue','white','white','mistyrose',\n",
    "                                            'lightcoral','indianred','brown','firebrick'])\n",
    "        cmapU850.set_over('maroon')\n",
    "        cmapU850.set_under('midnightblue')\n",
    "        u_ERA = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5/ua_ERA5.nc')\n",
    "        u_ERA = u_ERA.u.sel(level=850).sel(time=slice('1979','2018'))\n",
    "        u_ERA = u_ERA.rename({'longitude':'lon','latitude':'lat'})\n",
    "        u_ERA = u_ERA.groupby('time.season').mean(dim='time').sel(season='DJF')\n",
    "\n",
    "        fig_coef = plt.figure(figsize=(20, 16),dpi=100,constrained_layout=True)\n",
    "        projection_stereo = ccrs.SouthPolarStereo(central_longitude=300)\n",
    "        projection_plate = ccrs.PlateCarree(180)\n",
    "        data_crs = ccrs.PlateCarree()\n",
    "        for k in range(self.rd_num):\n",
    "            lat = maps[k].lat\n",
    "            lon = np.linspace(0,360,len(maps[k].lon))\n",
    "            var_c, lon_c = add_cyclic_point(maps[k].values,lon)\n",
    "            #SoutherHemisphere Stereographic for winds\n",
    "            if var == 'u':\n",
    "                ax = plt.subplot(3,2,k+1,projection=projection_stereo)\n",
    "                ax.set_extent([0,359.9, -90, 0], crs=data_crs)\n",
    "                theta = np.linspace(0, 2*np.pi, 100)\n",
    "                center, radius = [0.5, 0.5], 0.5\n",
    "                verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n",
    "                circle = mpath.Path(verts * radius + center)\n",
    "                ax.set_boundary(circle, transform=ax.transAxes)\n",
    "            #Plate Carree map for SST\n",
    "            elif var == 'sst':\n",
    "                ax = plt.subplot(3,2,k+1,projection=projection_plate)\n",
    "            else: \n",
    "                ax = plt.subplot(3,2,k+1,projection=projection_stereo)\n",
    "            if k == 6:\n",
    "                im0=ax.contourf(lon_c, lat, var_c,transform=data_crs,cmap='OrRd',extend='both')\n",
    "            else:\n",
    "                clevels = np.arange(-.6,.7,0.1)\n",
    "                im=ax.contourf(lon_c, lat, var_c,clevels,transform=data_crs,cmap='RdBu_r',extend='both')\n",
    "            cnt=ax.contour(u_ERA.lon,u_ERA.lat, u_ERA.values,levels=[8],transform=data_crs,linewidths=1.2, colors='black', linestyles='-')\n",
    "            plt.clabel(cnt,inline=True,fmt='%1.0f',fontsize=8)\n",
    "            if maps_pval[k].min() < 0.05: \n",
    "                try:\n",
    "                    levels = [maps_pval[k].min(),0.05,maps_pval[k].max()]\n",
    "                    ax.contourf(maps_pval[k].lon, lat, maps_pval[k].values,levels, transform=data_crs,levels=levels, hatches=[\"...\", \" \"], alpha=0)\n",
    "                except:\n",
    "                    levels = [maps_pval[k].min(),maps_pval[k].min()+0.01*maps_pval[k].min(),maps_pval[k].max()]\n",
    "                    ax.contourf(maps_pval[k].lon, lat, maps_pval[k].values,levels, transform=data_crs,levels=levels, hatches=[\"...\", \" \"], alpha=0)\n",
    "            elif maps_pval[k].min() < 0.50:\n",
    "                levels = [maps_pval[k].min(),0.50,maps_pval[k].max()]\n",
    "                ax.contourf(maps_pval[k].lon, lat, maps_pval[k].values,levels, transform=data_crs,levels=levels, hatches=[\"...\", \" \"], alpha=0)\n",
    "            else:\n",
    "                print('No significant values for ',self.regressor_names[k]) \n",
    "            plt.title(self.regressor_names[k],fontsize=18)\n",
    "            ax.add_feature(cartopy.feature.COASTLINE,alpha=.5)\n",
    "            ax.add_feature(cartopy.feature.BORDERS, linestyle='-', alpha=.5)\n",
    "            ax.gridlines(crs=data_crs, linewidth=0.3, linestyle='-')\n",
    "            if var == 'ua':\n",
    "                ax.set_extent([-180, 180, -90, -25], ccrs.PlateCarree())\n",
    "            elif var == 'sst':\n",
    "                ax.set_extent([-60, 220, -80, 40], ccrs.PlateCarree(central_longitude=180))\n",
    "            else: \n",
    "                ax.set_extent([-60, 220, -80, -25], ccrs.PlateCarree(central_longitude=180))\n",
    "            \n",
    "        plt1_ax = plt.gca()\n",
    "        left, bottom, width, height = plt1_ax.get_position().bounds\n",
    "        if var == 'ua':\n",
    "            colorbar_axes1 = fig_coef.add_axes([left+0.28, bottom, 0.01, height*2])\n",
    "            colorbar_axes2 = fig_coef.add_axes([left+0.36, bottom, 0.01, height*2])\n",
    "        elif var == 'sst':\n",
    "            colorbar_axes1 = fig_coef.add_axes([left+0.3, bottom, 0.01, height*3])\n",
    "            colorbar_axes2 = fig_coef.add_axes([left+0.38, bottom, 0.01, height*3])\n",
    "        else:\n",
    "            colorbar_axes1 = fig_coef.add_axes([left+0.28, bottom, 0.01, height*2])\n",
    "            colorbar_axes2 = fig_coef.add_axes([left+0.36, bottom, 0.01, height*2])\n",
    "        cbar = fig_coef.colorbar(im, colorbar_axes1, orientation='vertical')\n",
    "        cbar2 = fig_coef.colorbar(im, colorbar_axes2, orientation='vertical')\n",
    "        if var == 'ua':\n",
    "            cbar.set_label('m/s/std(rd)',fontsize=14) #rotation = radianes\n",
    "            cbar2.set_label('m/s/std(rd)',fontsize=14) #rotation = radianes\n",
    "        elif var == 'sst':\n",
    "            cbar.set_label('K/std(rd)',fontsize=14) #rotation = radianes\n",
    "            cbar2.set_label('K/std(rd)',fontsize=14) #rotation = radianes\n",
    "        else:\n",
    "            cbar.set_label('X/std(rd)',fontsize=14) #rotation = radianes\n",
    "            cbar2.set_label('X/std(rd)',fontsize=14) #rotation = radianes\n",
    "        cbar.ax.tick_params(axis='both',labelsize=14)\n",
    "        cbar2.ax.tick_params(axis='both',labelsize=14)\n",
    "            \n",
    "        plt.subplots_adjust(bottom=0.2, right=.95, top=0.8)\n",
    "        if var == 'ua':\n",
    "            plt.savefig(output_path+'/regression_coefficients_u850',bbox_inches='tight')\n",
    "        elif  var == 'sst':\n",
    "            plt.savefig(output_path+'/regression_coefficients_sst',bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(output_path+'/regression_coefficients_unknown_var',bbox_inches='tight')\n",
    "        \n",
    "        plt.clf\n",
    "\n",
    "        return fig_coef\n",
    "\n",
    "\n",
    "def plot_regression_coef_map_MEM(maps, maps_pval, regressor_names, output_path):\n",
    "    \"\"\"Plots figure with regression coefficient maps with two distinct colorbars.\n",
    "    :param regressor_names: list with strings naming the independent variables\n",
    "    :param path: saving path\n",
    "    :return: figure\n",
    "    \"\"\"\n",
    "\n",
    "    # Custom colormap\n",
    "    cmapU850 = mpl.colors.ListedColormap(['darkblue', 'navy', 'steelblue', 'lightblue',\n",
    "                                'lightsteelblue', 'white', 'white', 'mistyrose',\n",
    "                                'lightcoral', 'indianred', 'brown', 'firebrick'])\n",
    "    cmapU850.set_over('maroon')\n",
    "    cmapU850.set_under('midnightblue')\n",
    "\n",
    "    # Load data for contours\n",
    "    ua_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5/ua_ERA5.nc')\n",
    "    ua_era5 = ua_era5.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    ua_era5_850 = ua_era5.sel(level=850)\n",
    "    u_ERA = ua_era5_850.u.sel(time=slice('1979', '2018'))\n",
    "    u_ERA = u_ERA.groupby('time.season').mean(dim='time').sel(season='DJF')\n",
    "\n",
    "    # Create figure and subplots with adjusted size\n",
    "    fig_coef, axs = plt.subplots(2, 3, figsize=(24, 15), dpi=100,\n",
    "                                subplot_kw={'projection': ccrs.SouthPolarStereo(central_longitude=300)})\n",
    "    plt.subplots_adjust(bottom=0.1, right=0.85, top=0.85, hspace=0.1, wspace=0.25)\n",
    "\n",
    "    data_crs = ccrs.PlateCarree()\n",
    "\n",
    "    # Loop over the subplots\n",
    "    for k, ax in enumerate(axs.flat):\n",
    "        if k >= len(maps):  # Stop if we have more subplots than data\n",
    "            break\n",
    "        \n",
    "        lat = maps[k].lat\n",
    "        lon = np.linspace(0, 360, len(maps[k].lon))\n",
    "        var_c, lon_c = add_cyclic_point(maps[k].values, lon)\n",
    "\n",
    "        ax.set_extent([0, 359.9, -90, 0], crs=data_crs)\n",
    "        theta = np.linspace(0, 2 * np.pi, 100)\n",
    "        center, radius = [0.5, 0.5], 0.5\n",
    "        verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n",
    "        circle = mpath.Path(verts * radius + center)\n",
    "        ax.set_boundary(circle, transform=ax.transAxes)\n",
    "\n",
    "        # Use different color scales for the first and other subplots\n",
    "        if k == 0:\n",
    "            clevels = np.arange(0, 40, 5)\n",
    "            # Contour plot\n",
    "            im0 = ax.contourf(lon_c, maps[k].lat, var_c, clevels, transform=data_crs, cmap='OrRd', extend='both') #maps[k].values\n",
    "        else:\n",
    "            clevels = np.arange(-1, 1.1, 0.1)\n",
    "            # Contour plot\n",
    "            try:\n",
    "                im = ax.contourf(lon_c, maps[k].lat, var_c, clevels, transform=data_crs, cmap=cmapU850, extend='both')\n",
    "            except TypeError:\n",
    "                print(maps[k])\n",
    "\n",
    "        # Overlay contour lines for u_ERA\n",
    "        cnt = ax.contour(u_ERA.lon, u_ERA.lat, u_ERA.values, levels=[8], transform=data_crs,\n",
    "                        linewidths=1.2, colors='black', linestyles='-')\n",
    "        ax.clabel(cnt, inline=True, fmt='%1.0f', fontsize=8)\n",
    "\n",
    "        # Check for significant p-values and hatch regions\n",
    "        if (maps_pval[k].min() < 0.05) & (k > 0):\n",
    "            try:\n",
    "                levels = [maps_pval[k].min(), 0.05, maps_pval[k].max()]\n",
    "                ax.contourf(maps_pval[k].lon, lat, maps_pval[k].values, levels=levels,\n",
    "                            transform=data_crs, hatches=[\"...\", \" \"], alpha=0)\n",
    "            except:\n",
    "                levels = [maps_pval[k].min(), maps_pval[k].min()+maps_pval[k].min()*0.01, maps_pval[k].max()]\n",
    "                ax.contourf(maps_pval[k].lon, lat, maps_pval[k].values, levels=levels,\n",
    "                            transform=data_crs, hatches=[\"...\", \" \"], alpha=0)\n",
    "        \n",
    "\n",
    "        # Define box coordinates (example: covers part of North America)\n",
    "        lon_min, lon_max = 0, 360\n",
    "        lat_min, lat_max = -70, -30\n",
    "        width = lon_max - lon_min\n",
    "        height = lat_max - lat_min\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((lon_min, lat_min), width, height,\n",
    "                                linewidth=1, edgecolor='black', facecolor='none')\n",
    "\n",
    "        # Add rectangle to the map\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Define box coordinates (example: covers part of North America)\n",
    "        lon_min, lon_max = 120, 180\n",
    "        lat_min, lat_max = -70, -30\n",
    "        width = lon_max - lon_min\n",
    "        height = lat_max - lat_min\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((lon_min, lat_min), width, height,\n",
    "                                linewidth=1, edgecolor='red', facecolor='none')\n",
    "\n",
    "        # Add rectangle to the map\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Plot title\n",
    "        ax.set_title(regressor_names[k], fontsize=20)\n",
    "        \n",
    "        # Add coastlines and borders\n",
    "        ax.add_feature(cartopy.feature.COASTLINE, alpha=.5)\n",
    "        ax.add_feature(cartopy.feature.BORDERS, linestyle='-', alpha=.5)\n",
    "        ax.gridlines(crs=data_crs, linewidth=0.3, linestyle='-')\n",
    "        ax.set_extent([-180, 180, -90, -25], ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "    # Create two colorbars outside the grid of subplots\n",
    "\n",
    "    # Colorbar for the first subplot\n",
    "    cbar_ax_1 = fig_coef.add_axes([0.87, 0.55, 0.02, 0.25])  # Manually specify position\n",
    "    cbar_1 = fig_coef.colorbar(im0, cax=cbar_ax_1, orientation='vertical', ticks=np.arange(0, 40, 5))\n",
    "    cbar_1.set_label(r'${R}^{2}$', fontsize=16)\n",
    "    cbar_1.ax.tick_params(axis='both', labelsize=15)\n",
    "\n",
    "    # Add \"panel a\" text above the first colorbar\n",
    "    plt.text(0.87, 0.82, 'panel a', fontsize=16, transform=fig_coef.transFigure, ha='center')\n",
    "\n",
    "    # Colorbar for the remaining subplots\n",
    "    cbar_ax_2 = fig_coef.add_axes([0.87, 0.18, 0.02, 0.25])  # Manually specify position\n",
    "    cbar_2 = fig_coef.colorbar(im, cax=cbar_ax_2, orientation='vertical', ticks=np.arange(-1, 1.2, 0.2))\n",
    "    cbar_2.set_label(r'$\\sigma$ $\\sigma_{RD}^{-1}$', fontsize=16)\n",
    "    cbar_2.ax.tick_params(axis='both', labelsize=18)\n",
    "\n",
    "    # Add \"panels b-f\" text above the second colorbar\n",
    "    plt.text(0.87, 0.45, 'panels b-f', fontsize=15, transform=fig_coef.transFigure, ha='center')\n",
    "\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "    return fig_coef\n",
    "\n",
    "\n",
    "def plot_regression_coef_map_row(maps, maps_pval, regressor_names, output_path):\n",
    "    \"\"\"Plots figure with regression coefficient maps with two distinct colorbars.\n",
    "    :param regressor_names: list with strings naming the independent variables\n",
    "    :param path: saving path\n",
    "    :return: figure\n",
    "    \"\"\"\n",
    "\n",
    "    # Custom colormap\n",
    "    cmapU850 = mpl.colors.ListedColormap(['darkblue', 'navy', 'steelblue', 'lightblue',\n",
    "                                'lightsteelblue', 'white', 'white', 'mistyrose',\n",
    "                                'lightcoral', 'indianred', 'brown', 'firebrick'])\n",
    "    cmapU850.set_over('maroon')\n",
    "    cmapU850.set_under('midnightblue')\n",
    "\n",
    "    # Load data for contours\n",
    "    ua_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5/ua_ERA5.nc')\n",
    "    ua_era5 = ua_era5.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    ua_era5_850 = ua_era5.sel(level=850)\n",
    "    u_ERA = ua_era5_850.u.sel(time=slice('1979', '2018'))\n",
    "    u_ERA = u_ERA.groupby('time.season').mean(dim='time').sel(season='DJF')\n",
    "\n",
    "    # Create figure and subplots with adjusted size\n",
    "    fig_coef, axs = plt.subplots(1, 6, figsize=(30, 10), dpi=100,\n",
    "                                subplot_kw={'projection': ccrs.SouthPolarStereo(central_longitude=300)})\n",
    "    plt.subplots_adjust(bottom=0.1, right=0.85, top=0.85, hspace=0.1, wspace=0.25)\n",
    "\n",
    "    data_crs = ccrs.PlateCarree()\n",
    "\n",
    "    # Loop over the subplots\n",
    "    for k, ax in enumerate(axs.flat):\n",
    "        if k >= len(maps):  # Stop if we have more subplots than data\n",
    "            break\n",
    "        \n",
    "        lat = maps[k].lat\n",
    "        lon = np.linspace(0, 360, len(maps[k].lon))\n",
    "        var_c, lon_c = add_cyclic_point(maps[k].values, lon)\n",
    "\n",
    "        ax.set_extent([0, 359.9, -90, 0], crs=data_crs)\n",
    "        theta = np.linspace(0, 2 * np.pi, 100)\n",
    "        center, radius = [0.5, 0.5], 0.5\n",
    "        verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n",
    "        circle = mpath.Path(verts * radius + center)\n",
    "        ax.set_boundary(circle, transform=ax.transAxes)\n",
    "\n",
    "        # Use different color scales for the first and other subplots\n",
    "        if k == 0:\n",
    "            clevels = np.arange(0, 35, 5)\n",
    "            # Contour plot\n",
    "            im0 = ax.contourf(lon_c, maps[k].lat, var_c, clevels, transform=data_crs, cmap='OrRd', extend='both') #maps[k].values\n",
    "        else:\n",
    "            clevels = np.arange(-1, 1.1, 0.1)\n",
    "            # Contour plot\n",
    "            try:\n",
    "                im = ax.contourf(lon_c, maps[k].lat, var_c, clevels, transform=data_crs, cmap=cmapU850, extend='both')\n",
    "            except TypeError:\n",
    "                print(maps[k])\n",
    "\n",
    "        # Overlay contour lines for u_ERA\n",
    "        cnt = ax.contour(u_ERA.lon, u_ERA.lat, u_ERA.values, levels=[8], transform=data_crs,\n",
    "                        linewidths=1.2, colors='black', linestyles='-')\n",
    "        ax.clabel(cnt, inline=True, fmt='%1.0f', fontsize=8)\n",
    "\n",
    "        # Check for significant p-values and hatch regions\n",
    "        if (maps_pval[k].min() < 0.05) & (k > 0):\n",
    "            try:\n",
    "                levels = [maps_pval[k].min(), 0.05, maps_pval[k].max()]\n",
    "                ax.contourf(maps_pval[k].lon, lat, maps_pval[k].values, levels=levels,\n",
    "                            transform=data_crs, hatches=[\"...\", \" \"], alpha=0)\n",
    "            except:\n",
    "                levels = [maps_pval[k].min(), maps_pval[k].min()+maps_pval[k].min()*0.01, maps_pval[k].max()]\n",
    "                ax.contourf(maps_pval[k].lon, lat, maps_pval[k].values, levels=levels,\n",
    "                            transform=data_crs, hatches=[\"...\", \" \"], alpha=0)\n",
    "        \n",
    "\n",
    "        # Define box coordinates (example: covers part of North America)\n",
    "        lon_min, lon_max = 0, 360\n",
    "        lat_min, lat_max = -70, -30\n",
    "        width = lon_max - lon_min\n",
    "        height = lat_max - lat_min\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((lon_min, lat_min), width, height,\n",
    "                                linewidth=1, edgecolor='black', facecolor='none')\n",
    "\n",
    "        # Add rectangle to the map\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Define box coordinates (example: covers part of North America)\n",
    "        lon_min, lon_max = 120, 180\n",
    "        lat_min, lat_max = -70, -30\n",
    "        width = lon_max - lon_min\n",
    "        height = lat_max - lat_min\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((lon_min, lat_min), width, height,\n",
    "                                linewidth=1, edgecolor='red', facecolor='none')\n",
    "\n",
    "        # Add rectangle to the map\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Plot title\n",
    "        ax.set_title(regressor_names[k], fontsize=20)\n",
    "        \n",
    "        # Add coastlines and borders\n",
    "        ax.add_feature(cartopy.feature.COASTLINE, alpha=.5)\n",
    "        ax.add_feature(cartopy.feature.BORDERS, linestyle='-', alpha=.5)\n",
    "        ax.gridlines(crs=data_crs, linewidth=0.3, linestyle='-')\n",
    "        ax.set_extent([-180, 180, -90, -25], ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "    # Create two colorbars outside the grid of subplots\n",
    "\n",
    "    # Colorbar for the first subplot\n",
    "    #cbar_ax_1 = fig_coef.add_axes([0.8, 0.55, 0.02, 0.25])  # Manually specify position\n",
    "    cbar_1 = fig_coef.colorbar(im0, orientation='horizontal', ticks=np.arange(0, 40, 5))\n",
    "    cbar_1.set_label(r'${R}^{2}$', fontsize=16)\n",
    "    cbar_1.ax.tick_params(axis='both', labelsize=15)\n",
    "\n",
    "    # Colorbar for the remaining subplots\n",
    "    #cbar_ax_2 = fig_coef.add_axes([0.88, 0.55, 0.02, 0.25])  # Manually specify position\n",
    "    cbar_2 = fig_coef.colorbar(im, orientation='horizontal', ticks=np.arange(-1, 1.2, 0.2))\n",
    "    cbar_2.set_label(r'$\\sigma$ $\\sigma_{RD}^{-1}$', fontsize=16)\n",
    "    cbar_2.ax.tick_params(axis='both', labelsize=18)\n",
    "\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "    return fig_coef\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import ERA5 data\n",
    "\n",
    "ua_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5/ua_ERA5.nc')\n",
    "ua_era5 = ua_era5.rename({'latitude':'lat','longitude':'lon'})\n",
    "ua_era5_50 = ua_era5.sel(level=50)\n",
    "ua_era5_850 = ua_era5.sel(level=850)\n",
    "ta_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5/ta_ERA5.nc')\n",
    "ta_era5 = ta_era5.rename({'latitude':'lat','longitude':'lon'})\n",
    "\n",
    "### Import NCEP data\n",
    "\n",
    "ua_ncep = xr.open_dataset('/home/jmindlin/causal_EDJ/NCEP/uwnd.mon.mean.nc')\n",
    "ua_ncep_50 = ua_ncep.sel(level=50)\n",
    "ua_ncep_850 = ua_ncep.sel(level=850)\n",
    "ta_ncep = xr.open_dataset('/home/jmindlin/causal_EDJ/NCEP/air.mon.mean.nc')\n",
    "\n",
    "del ua_era5, ua_ncep\n",
    "\n",
    "### Import JRA55 data\n",
    "\n",
    "ua_jra55_50 = [xr.open_dataset('/home/jmindlin/causal_EDJ/JRA55/ua/anl_mdl.033_ugrd.reg_tl319.'+str(year)+'01_'+str(year)+'12.mindlin756630_50hPa.nc') for year in np.arange(1958,2024,1)]\n",
    "ua_jra55_50_concat = xr.concat(ua_jra55_50,dim='initial_time0_hours')\n",
    "ua_jra55_50_concat = ua_jra55_50_concat.rename({'initial_time0_hours':'time','g4_lat_2':'lat','g4_lon_3':'lon'})\n",
    "\n",
    "ua_jra55_850 = [xr.open_dataset('/home/jmindlin/causal_EDJ/JRA55/ua/anl_mdl.033_ugrd.reg_tl319.'+str(year)+'01_'+str(year)+'12.mindlin756630_847hPa.nc') for year in np.arange(1958,2024,1)]\n",
    "ua_jra55_850_concat = xr.concat(ua_jra55_850,dim='initial_time0_hours')\n",
    "ua_jra55_850_concat = ua_jra55_850_concat.rename({'initial_time0_hours':'time','g4_lat_2':'lat','g4_lon_3':'lon'})\n",
    "\n",
    "ta_jra55 = [xr.open_dataset('/home/jmindlin/causal_EDJ/JRA55/ta/anl_mdl.011_tmp.reg_tl319.'+str(year)+'01_'+str(year)+'12.mindlin754486.nc') for year in np.arange(1958,2024,1)]\n",
    "ta_jra55_concat = xr.concat(ta_jra55,dim='initial_time0_hours')\n",
    "ta_jra55_concat = ta_jra55_concat.rename({'initial_time0_hours':'time','g4_lat_2':'lat','g4_lon_3':'lon','lv_HYBL1':'lev'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.675 -0.333 -0.591 ... -9.999 -9.999 -9.999]\n"
     ]
    }
   ],
   "source": [
    "def regressor_EESC_GW(gw_ts):\n",
    "    eesc_ts = pd.read_csv('/home/jmindlin/causal_EDJ/send_to_LIM/GW_EESC_polar_ozoneloss.csv')\n",
    "    for i in range(len(eesc_ts[:8])):\n",
    "        eesc_ts['EESC_polar'][i] = eesc_ts['EESC_polar'][8]\n",
    "    df = pd.DataFrame({'EESC':eesc_ts['EESC_polar'][10:79] - eesc_ts['EESC_polar'][8],'GW':gw_ts})\n",
    "    regressors_out = sm.add_constant(df.values)\n",
    "    return regressors_out, df\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# URL of the data file\n",
    "url = \"https://crudata.uea.ac.uk/cru/data/temperature/HadCRUT5.0Analysis_gl.txt\"\n",
    "\n",
    "# Fetch the data from the URL\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    lines = response.read().decode('utf-8').splitlines()\n",
    "\n",
    "# Parse the lines to extract the data\n",
    "data = []\n",
    "months = []\n",
    "years = []\n",
    "for line in lines[::2]:\n",
    "    values = line.split(' ')[2:-1]\n",
    "    years.append(line.split(' ')[1])\n",
    "    for i, value in enumerate(values):\n",
    "        if value != '':\n",
    "            data.append(value)\n",
    "            months.append(i)\n",
    "\n",
    "# Convert the list of lists into a NumPy array\n",
    "data_array = np.array(data, dtype=float)\n",
    "data_array = data_array\n",
    "\n",
    "# Print the resulting NumPy array\n",
    "print(data_array)\n",
    "\n",
    "time = pd.date_range(start='1850-01-01', end='2024-12-01', freq='MS')\n",
    "temperature_data = xr.DataArray(\n",
    "    data_array, \n",
    "    coords={'time': time}, \n",
    "    dims='time', \n",
    "    name='temperature - HadCRU5'\n",
    ")\n",
    "\n",
    "tas_DJF = seasonal_data_months(temperature_data.sel(time=slice('1950','2023')),[12,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_1979_2019 = xr.open_dataset('/home/jmindlin/causal_EDJ/send_to_LIM/ERA5/era5.mon.mean_T42.nc').u.sel(lev=50).drop_vars('lev')\n",
    "u_1950_1978 = xr.open_dataset('/home/jmindlin/causal_EDJ/send_to_LIM/ERA5/ERA5_monthly_u_wind_n36_rename_regrid.nc').u.sel(plev=5000).drop_vars('plev')\n",
    "u_1950_2019 = xr.concat([u_1950_1978,u_1979_2019],'time')\n",
    "spv_era5_OND = seasonal_data_months(u_1950_2019,[10,11,12]).sel(lat=slice(-50,-60)).mean(dim='lat').mean(dim='lon').sel(time=slice('1950','2019'))\n",
    "spv_era5_1950_2019_OND = spv_era5_OND\n",
    "CLIM_spv_era5 = spv_era5_1950_2019_OND.sel(time=slice('1950','2019')).mean(dim='time')\n",
    "\n",
    "spv_ncep_OND = seasonal_data_months(ua_ncep_50,[10,11,12]).sel(lat=slice(-50,-60)).mean(dim='lat').mean(dim='lon').sel(time=slice('1950','2023'))\n",
    "spv_ncep_1950_2023_OND = spv_ncep_OND\n",
    "CLIM_spv_ncep = spv_ncep_1950_2023_OND.sel(time=slice('1950','2023')).mean(dim='time')\n",
    "\n",
    "spv_jra55_OND = seasonal_data_months(ua_jra55_50_concat,[10,11,12]).sel(lat=slice(-50,-60)).mean(dim='lat').mean(dim='lon').sel(time=slice('1950','2023'))\n",
    "spv_jra55_1950_2023_OND = spv_jra55_OND\n",
    "CLIM_spv_jra55 = spv_jra55_1950_2023_OND.sel(time=slice('1950','2023')).mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extend ERA5 SPV 2019-2023 with NCEP data\n",
    "spv_era5_1950_2023_OND = xr.concat([spv_era5_1950_2019_OND,spv_ncep_1950_2023_OND.uwnd.sel(time=slice('2020','2023'))],dim='time')\n",
    "\n",
    "stratospheric_polar_vortex_rean = []\n",
    "stratospheric_polar_vortex_rean.append(spv_era5_1950_2023_OND)\n",
    "stratospheric_polar_vortex_rean.append(spv_ncep_1950_2023_OND)\n",
    "stratospheric_polar_vortex_rean.append(spv_jra55_1950_2023_OND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tropical_warming = []\n",
    "tw_era5_DJF = seasonal_data_months(ta_era5,[12,1,2]).sel(lat=slice(15,-15)).mean(dim='lat').mean(dim='lon').sel(time=slice('1950','2023'))\n",
    "tw_era5_1950_2023_DJF = tw_era5_DJF\n",
    "tropical_warming.append(tw_era5_DJF)\n",
    "CLIM_tw_ncep = tw_era5_1950_2023_DJF.sel(time=slice('1950','2023')).mean(dim='time')\n",
    "\n",
    "tw_ncep_DJF = seasonal_data_months(ta_ncep,[12,1,2]).sel(level=250).sel(lat=slice(15,-15)).mean(dim='lat').mean(dim='lon').sel(time=slice('1950','2023')) + 273.15\n",
    "tw_ncep_1950_2023_DJF = tw_ncep_DJF\n",
    "tropical_warming.append(tw_ncep_DJF)\n",
    "CLIM_tw_ncep = tw_ncep_1950_2023_DJF.sel(time=slice('1950','2023')).mean(dim='time')\n",
    "\n",
    "tw_jra55_DJF = seasonal_data_months(ta_jra55_concat,[12,1,2]).sel(lev=29).sel(lat=slice(15,-15)).mean(dim='lat').mean(dim='lon').sel(time=slice('1950','2023'))\n",
    "tw_jra55_1950_2023_DJF = tw_jra55_DJF\n",
    "tropical_warming.append(tw_jra55_DJF)\n",
    "CLIM_tw_jra55 = tw_jra55_1950_2023_DJF.sel(time=slice('1950','2023')).mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SST data\n",
    "sst_ERSST = xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mnmean_ERSST_2022_KAPLAN_grid.nc') #- xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mnmean_ERSST_2022_KAPLAN_grid.nc').mean(dim='lon')\n",
    "sst_COBE = xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mon.mean_COBE_2022_KAPLAN_grid.nc')# - xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mon.mean_COBE_2022_KAPLAN_grid.nc').mean(dim='lon')\n",
    "sst_HadISST = xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/HadISST_sst_latest_KAPLAN_grid.nc') #- xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/HadISST_sst_latest_KAPLAN_grid.nc').mean(dim='lon')\n",
    "sst_Kaplan = xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mean.anom_Kaplan_2022_KAPLAN_grid.nc') #- xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mean.anom_Kaplan_2022_KAPLAN_grid.nc').mean(dim='lon')\n",
    "\n",
    "sst_ERSST_CP = sst_ERSST.sel(lon=slice(180,250)).sst.sel(lat=slice(-5,5)).mean(dim='lat').mean(dim='lon') \n",
    "sst_ERSST_CP_DJF = seasonal_data_months(sst_ERSST_CP,[12,1,2])\n",
    "#sst_ERSST_CP_DJF = sst_ERSST_CP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_ERSST_EP = sst_ERSST.sel(lon=slice(260,280)).sst.sel(lat=slice(0,10)).mean(dim='lat').mean(dim='lon')\n",
    "sst_ERSST_EP_DJF = seasonal_data_months(sst_ERSST_EP,[12,1,2])\n",
    "#sst_ERSST_EP_DJF = sst_ERSST_EP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_COBE_CP = sst_COBE.sel(lon=slice(180,250)).sst.sel(lat=slice(-5,5)).mean(dim='lat').mean(dim='lon')\n",
    "sst_COBE_CP_DJF = seasonal_data_months(sst_COBE_CP,[12,1,2])\n",
    "#sst_COBE_CP_DJF = sst_COBE_CP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_COBE_EP = sst_COBE.sel(lon=slice(260,280)).sst.sel(lat=slice(0,10)).mean(dim='lat').mean(dim='lon')\n",
    "sst_COBE_EP_DJF = seasonal_data_months(sst_COBE_EP,[12,1,2])\n",
    "#sst_COBE_EP_DJF = sst_COBE_EP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_HadISST_CP = sst_HadISST.sel(lon=slice(180,250)).sst.sel(lat=slice(-5,5)).mean(dim='lat').mean(dim='lon')\n",
    "sst_HadISST_CP_DJF = seasonal_data_months(sst_HadISST_CP,[12,1,2])\n",
    "#sst_HadISST_CP_DJF = sst_HadISST_CP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_HadISST_EP = sst_HadISST.sel(lon=slice(260,280)).sst.sel(lat=slice(0,10)).mean(dim='lat').mean(dim='lon')\n",
    "sst_HadISST_EP_DJF = seasonal_data_months(sst_HadISST_EP,[12,1,2])\n",
    "#sst_HadISST_EP_DJF = sst_HadISST_EP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_Kaplan_CP = sst_Kaplan.sel(lon=slice(180,250)).sst.sel(lat=slice(-5,5)).mean(dim='lat').mean(dim='lon')\n",
    "sst_Kaplan_CP_DJF = seasonal_data_months(sst_Kaplan_CP,[12,1,2])\n",
    "#sst_Kaplan_CP_DJF = sst_Kaplan_CP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_Kaplan_EP = sst_Kaplan.sel(lon=slice(260,280)).sst.sel(lat=slice(0,10)).mean(dim='lat').mean(dim='lon')\n",
    "sst_Kaplan_EP_DJF = seasonal_data_months(sst_Kaplan_EP,[12,1,2])\n",
    "#sst_Kaplan_EP_DJF = sst_Kaplan_EP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_CP_obs = [sst_ERSST_CP_DJF,sst_COBE_CP_DJF,sst_HadISST_CP_DJF,sst_Kaplan_CP_DJF]\n",
    "sst_EP_obs = [sst_ERSST_EP_DJF,sst_COBE_EP_DJF,sst_HadISST_EP_DJF,sst_Kaplan_EP_DJF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SST data\n",
    "sst_ERSST_asym = xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mnmean_ERSST_2022_KAPLAN_grid.nc') - xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mnmean_ERSST_2022_KAPLAN_grid.nc').mean(dim='lon')\n",
    "sst_COBE_asym = xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mon.mean_COBE_2022_KAPLAN_grid.nc') - xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mon.mean_COBE_2022_KAPLAN_grid.nc').mean(dim='lon')\n",
    "sst_HadISST_asym = xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/HadISST_sst_latest_KAPLAN_grid.nc') - xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/HadISST_sst_latest_KAPLAN_grid.nc').mean(dim='lon')\n",
    "sst_Kaplan_asym = xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mean.anom_Kaplan_2022_KAPLAN_grid.nc') - xr.open_dataset('/home/jmindlin/causal_EDJ/SST_data/sst.mean.anom_Kaplan_2022_KAPLAN_grid.nc').mean(dim='lon')\n",
    "\n",
    "sst_ERSST_CP_asym = sst_ERSST_asym.sel(lon=slice(180,250)).sst.sel(lat=slice(-5,5)).mean(dim='lat').mean(dim='lon') \n",
    "sst_ERSST_CP_DJF_asym = seasonal_data_months(sst_ERSST_CP_asym,[12,1,2])\n",
    "#sst_ERSST_CP_DJF = sst_ERSST_CP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_ERSST_EP_asym = sst_ERSST_asym.sel(lon=slice(260,280)).sst.sel(lat=slice(0,10)).mean(dim='lat').mean(dim='lon')\n",
    "sst_ERSST_EP_DJF_asym = seasonal_data_months(sst_ERSST_EP_asym,[12,1,2])\n",
    "#sst_ERSST_EP_DJF = sst_ERSST_EP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_COBE_CP_asym = sst_COBE_asym.sel(lon=slice(180,250)).sst.sel(lat=slice(-5,5)).mean(dim='lat').mean(dim='lon')\n",
    "sst_COBE_CP_DJF_asym = seasonal_data_months(sst_COBE_CP_asym,[12,1,2])\n",
    "#sst_COBE_CP_DJF = sst_COBE_CP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_COBE_EP_asym = sst_COBE_asym.sel(lon=slice(260,280)).sst.sel(lat=slice(0,10)).mean(dim='lat').mean(dim='lon')\n",
    "sst_COBE_EP_DJF_asym = seasonal_data_months(sst_COBE_EP_asym,[12,1,2])\n",
    "#sst_COBE_EP_DJF = sst_COBE_EP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_HadISST_CP_asym = sst_HadISST_asym.sel(lon=slice(180,250)).sst.sel(lat=slice(-5,5)).mean(dim='lat').mean(dim='lon')\n",
    "sst_HadISST_CP_DJF_asym = seasonal_data_months(sst_HadISST_CP_asym,[12,1,2])\n",
    "#sst_HadISST_CP_DJF = sst_HadISST_CP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_HadISST_EP_asym = sst_HadISST_asym.sel(lon=slice(260,280)).sst.sel(lat=slice(0,10)).mean(dim='lat').mean(dim='lon')\n",
    "sst_HadISST_EP_DJF_asym = seasonal_data_months(sst_HadISST_EP_asym,[12,1,2])\n",
    "#sst_HadISST_EP_DJF = sst_HadISST_EP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_Kaplan_CP_asym = sst_Kaplan_asym.sel(lon=slice(180,250)).sst.sel(lat=slice(-5,5)).mean(dim='lat').mean(dim='lon')\n",
    "sst_Kaplan_CP_DJF_asym = seasonal_data_months(sst_Kaplan_CP_asym,[12,1,2])\n",
    "#sst_Kaplan_CP_DJF = sst_Kaplan_CP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_Kaplan_EP_asym = sst_Kaplan_asym.sel(lon=slice(260,280)).sst.sel(lat=slice(0,10)).mean(dim='lat').mean(dim='lon')\n",
    "sst_Kaplan_EP_DJF_asym = seasonal_data_months(sst_Kaplan_EP_asym,[12,1,2])\n",
    "#sst_Kaplan_EP_DJF = sst_Kaplan_EP_DJF.sel(time=slice('1950','2018'))\n",
    "\n",
    "sst_CP_obs_asym = [sst_ERSST_CP_DJF_asym,sst_COBE_CP_DJF_asym,sst_HadISST_CP_DJF_asym,sst_Kaplan_CP_DJF_asym]\n",
    "sst_EP_obs_asym = [sst_ERSST_EP_DJF_asym,sst_COBE_EP_DJF_asym,sst_HadISST_EP_DJF_asym,sst_Kaplan_EP_DJF_asym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_era5_850_djf = seasonal_data(ua_era5_850,'DJF')\n",
    "ua_ncep_850_djf = seasonal_data(ua_ncep_850,'DJF')\n",
    "ua_jra55_850_concat_djf = seasonal_data(ua_jra55_850_concat,'DJF')\n",
    "\n",
    "era5_lat, era5_str = jet_lat_strength(ua_era5_850_djf.u)\n",
    "ncep_lat, ncep_str = jet_lat_strength(ua_ncep_850_djf.uwnd)\n",
    "jra55_lat, jra55_str = jet_lat_strength(ua_jra55_850_concat_djf.UGRD_GDS4_HYBL_S123)\n",
    "\n",
    "era5_lat_zm, era5_str_zm = jet_lat_strength(ua_era5_850_djf.u,0,360)\n",
    "ncep_lat_zm, ncep_str_zm = jet_lat_strength(ua_ncep_850_djf.uwnd,0,360)\n",
    "jra55_lat_zm, jra55_str_zm = jet_lat_strength(ua_jra55_850_concat_djf.UGRD_GDS4_HYBL_S123,0,360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pressure Sea Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp = xr.open_dataset('/home/jmindlin/causal_EDJ/NCEP/slp.mon.mean.nc')\n",
    "slp_DJF = seasonal_data_months(slp,[12,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is regressor_coefs: <xarray.Dataset>\n",
      "Dimensions:  (lat: 73, lon: 144)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0\n",
      "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "Data variables:\n",
      "    const    (lat, lon) float64 -7.133e-09 -7.133e-09 ... -7.3e-09 -7.3e-09\n",
      "    spv      (lat, lon) float64 0.07049 0.07049 0.07049 ... -0.03163 -0.03163\n",
      "    tw       (lat, lon) float64 0.2715 0.2715 0.2715 ... -0.09167 -0.09167\n",
      "    cp       (lat, lon) float64 -0.1012 -0.1012 -0.1012 ... 0.2586 0.2586 0.2586\n",
      "    ep       (lat, lon) float64 -0.0301 -0.0301 -0.0301 ... -0.1052 -0.1052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PSL - ERA5\n",
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(detrend_timeseries(slp_DJF.slp.sel(lat=slice(90,-90)).sel(time=slice('1950','2023'))))  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(detrend_timeseries(stratospheric_polar_vortex_rean[1].uwnd.sel(time=slice('1950','2023')))),\n",
    "                           'tw': standardize_data(detrend_timeseries(tropical_warming[1].air.sel(time=slice('1950','2023')))),\n",
    "                           'cp': standardize_data(detrend_timeseries(sst_CP_obs[0].sel(time=slice('1950','2023')))),\n",
    "                           'ep': standardize_data(detrend_timeseries(sst_EP_obs[0].sel(time=slice('1950','2023'))))})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'era5')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/without_GMST','psl')\n",
    "\n",
    "# Plot results\n",
    "regressor_names = ['const','spv', 'tw','cp','ep']  # Regressor names\n",
    "maps_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/without_GMST/psl/regression_coefficients_era5.nc')\n",
    "maps_era5_pvals = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/without_GMST/psl/regression_coefficients_pvalues_era5.nc')\n",
    "maps = [maps_era5[rd] for rd in regressor_names]\n",
    "maps_pvalues = [maps_era5_pvals[rd] for rd in regressor_names]\n",
    "titles = ['Climatology','Stratospheric \\n Polar Vortex','Tropial Upper Tropospheric \\n Temperature','Central Pacific SST','Eastern Pacific SST','Global Mean \\n Surface Temperature']\n",
    "fig = plot_regression_coef_map_MEM(maps, maps_pvalues, titles, '/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/without_GMST/psl/ERA5_detrend_without_GMST_psl_coef.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is regressor_coefs: <xarray.Dataset>\n",
      "Dimensions:  (lat: 73, lon: 144)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0\n",
      "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "Data variables:\n",
      "    const    (lat, lon) float64 -1.052e-08 -1.052e-08 ... -7.944e-09 -7.944e-09\n",
      "    spv      (lat, lon) float64 0.1087 0.1087 0.1087 ... -0.02437 -0.02437\n",
      "    tw       (lat, lon) float64 0.5696 0.5696 0.5696 ... -0.03506 -0.03506\n",
      "    cp       (lat, lon) float64 -0.1611 -0.1611 -0.1611 ... 0.2473 0.2473 0.2473\n",
      "    ep       (lat, lon) float64 -0.0008328 -0.0008328 ... -0.0996 -0.0996\n",
      "    gmst     (lat, lon) float64 -0.3527 -0.3527 -0.3527 ... -0.06697 -0.06697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PSL - ERA5\n",
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(detrend_timeseries(slp_DJF.slp.sel(lat=slice(90,-90)).sel(time=slice('1950','2023'))))  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(detrend_timeseries(stratospheric_polar_vortex_rean[1].uwnd.sel(time=slice('1950','2023')))),\n",
    "                           'tw': standardize_data(detrend_timeseries(tropical_warming[1].air.sel(time=slice('1950','2023')))),\n",
    "                           'cp': standardize_data(detrend_timeseries(sst_CP_obs[0].sel(time=slice('1950','2023')))),\n",
    "                           'ep': standardize_data(detrend_timeseries(sst_EP_obs[0].sel(time=slice('1950','2023')))),\n",
    "                           'gmst': standardize_data(detrend_timeseries(tas_DJF.sel(time=slice('1950','2023'))))})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep','gmst']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'era5')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST','psl')\n",
    "\n",
    "# Plot results\n",
    "regressor_names = ['const','spv', 'tw','cp','ep','gmst']  # Regressor names\n",
    "maps_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/psl/regression_coefficients_era5.nc')\n",
    "maps_era5_pvals = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/psl/regression_coefficients_pvalues_era5.nc')\n",
    "maps = [maps_era5[rd] for rd in regressor_names]\n",
    "maps_pvalues = [maps_era5_pvals[rd] for rd in regressor_names]\n",
    "titles = ['Climatology','Stratospheric \\n Polar Vortex','Tropial Upper Tropospheric \\n Temperature','Central Pacific SST','Eastern Pacific SST','Global Mean \\n Surface Temperature']\n",
    "fig = plot_regression_coef_map_MEM(maps, maps_pvalues, titles, '/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/psl/ERA5_detrend_with_GMST_psl_coef.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is regressor_coefs: <xarray.Dataset>\n",
      "Dimensions:  (lat: 73, lon: 144)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0\n",
      "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "Data variables:\n",
      "    const    (lat, lon) float64 -3.098e-05 -3.098e-05 ... -6.246e-05 -6.246e-05\n",
      "    spv      (lat, lon) float64 0.06934 0.06934 0.06934 ... -0.004681 -0.004681\n",
      "    tw       (lat, lon) float64 0.4533 0.4533 0.4533 ... 0.03807 0.03807 0.03807\n",
      "    cp       (lat, lon) float64 -0.2132 -0.2132 -0.2132 ... 0.2576 0.2576 0.2576\n",
      "    ep       (lat, lon) float64 0.05377 0.05377 0.05377 ... -0.1247 -0.1247\n",
      "    gmst     (lat, lon) float64 -0.3053 -0.3053 -0.3053 ... -0.3594 -0.3594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PSL - ERA5\n",
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(slp_DJF.sel(lat=slice(90,-90)).sel(time=slice('1950','2023')).slp)  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(stratospheric_polar_vortex_rean[1].uwnd.sel(time=slice('1950','2023'))),\n",
    "                           'tw': standardize_data(tropical_warming[1].air.sel(time=slice('1950','2023'))),\n",
    "                           'cp': standardize_data(sst_CP_obs[0].sel(time=slice('1950','2023'))),\n",
    "                           'ep': standardize_data(sst_EP_obs[0].sel(time=slice('1950','2023'))),\n",
    "                           'gmst': standardize_data(tas_DJF.sel(time=slice('1950','2023')))})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep','gmst']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'era5')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/with_GMST','psl')\n",
    "\n",
    "# Plot results\n",
    "regressor_names = ['const','spv', 'tw','cp','ep','gmst']  # Regressor names\n",
    "maps_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/with_GMST/psl/regression_coefficients_era5.nc')\n",
    "maps_era5_pvals = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/with_GMST/psl/regression_coefficients_pvalues_era5.nc')\n",
    "maps = [maps_era5[rd] for rd in regressor_names]\n",
    "maps_pvalues = [maps_era5_pvals[rd] for rd in regressor_names]\n",
    "titles = ['Climatology','Stratospheric \\n Polar Vortex','Tropical Upper Tropospheric \\n Temperature','Central Pacific SST','Eastern Pacific SST','Global Mean \\n Surface Temperature']\n",
    "fig = plot_regression_coef_map_MEM(maps, maps_pvalues, titles, '/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/with_GMST/psl/ERA5_trend_with_GMST_psl_coef.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is regressor_coefs: <xarray.Dataset>\n",
      "Dimensions:  (lat: 73, lon: 144)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0\n",
      "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "Data variables:\n",
      "    const    (lat, lon) float64 -2.543e-05 -2.543e-05 ... -5.592e-05 -5.592e-05\n",
      "    spv      (lat, lon) float64 0.1018 0.1018 0.1018 ... 0.03352 0.03352 0.03352\n",
      "    tw       (lat, lon) float64 0.2202 0.2202 0.2202 ... -0.2363 -0.2363 -0.2363\n",
      "    cp       (lat, lon) float64 0.07038 0.07038 0.07038 ... 0.5915 0.5915 0.5915\n",
      "    ep       (lat, lon) float64 -0.1754 -0.1754 -0.1754 ... -0.3945 -0.3945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PSL - ERA5\n",
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(slp_DJF.sel(lat=slice(90,-90)).sel(time=slice('1950','2023')).slp)  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(stratospheric_polar_vortex_rean[1].uwnd.sel(time=slice('1950','2023'))),\n",
    "                           'tw': standardize_data(tropical_warming[1].air.sel(time=slice('1950','2023'))),\n",
    "                           'cp': standardize_data(sst_CP_obs[0].sel(time=slice('1950','2023'))),\n",
    "                           'ep': standardize_data(sst_EP_obs[0].sel(time=slice('1950','2023')))})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'era5')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/without_GMST','psl')\n",
    "\n",
    "# Plot results\n",
    "regressor_names = ['const','spv', 'tw','cp','ep']  # Regressor names\n",
    "maps_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/without_GMST/psl/regression_coefficients_era5.nc')\n",
    "maps_era5_pvals = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/without_GMST/psl/regression_coefficients_pvalues_era5.nc')\n",
    "maps = [maps_era5[rd] for rd in regressor_names]\n",
    "maps_pvalues = [maps_era5_pvals[rd] for rd in regressor_names]\n",
    "titles = ['Climatology','Stratospheric \\n Polar Vortex','Tropical Upper Tropospheric \\n Temperature','Central Pacific SST','Eastern Pacific SST','Global Mean \\n Surface Temperature']\n",
    "fig = plot_regression_coef_map_MEM(maps, maps_pvalues, titles, '/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/without_GMST/psl/ERA5_trend_without_GMST_psl_coef.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Westerly winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U850 - ERA5\n",
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(ua_era5_850_djf.sel(lat=slice(0,-90)).sel(time=slice('1950','2023')).u)  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(stratospheric_polar_vortex_rean[0].sel(time=slice('1950','2023'))),\n",
    "                           'tw': standardize_data(tropical_warming[0].t.sel(time=slice('1950','2023'))),\n",
    "                           'cp': standardize_data(sst_CP_obs[0].sel(time=slice('1950','2023'))),\n",
    "                           'ep': standardize_data(sst_EP_obs[0].sel(time=slice('1950','2023')))})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'era5')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/with_GMST','u850')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5 - U850\n",
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(detrend_timeseries(ua_era5_850_djf.sel(lat=slice(0,-90)).sel(time=slice('1950','2023')).u))  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(detrend_timeseries(stratospheric_polar_vortex_rean[0].sel(time=slice('1950','2023')))), \n",
    "                           'tw': standardize_data(detrend_timeseries(tropical_warming[0].t.sel(time=slice('1950','2023')))), \n",
    "                           'cp': standardize_data(detrend_timeseries(sst_CP_obs[0].sel(time=slice('1950','2023')))), \n",
    "                           'ep': standardize_data(detrend_timeseries(sst_EP_obs[0].sel(time=slice('1950','2023')))),\n",
    "                           'gmst': standardize_data(detrend_timeseries(tas_DJF.sel(time=slice('1950','2023')))),})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep','gmst']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'era5')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST','u850')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is regressor_coefs: <xarray.Dataset>\n",
      "Dimensions:  (lon: 1440, lat: 361)\n",
      "Coordinates:\n",
      "  * lon      (lon) float32 -180.0 -179.8 -179.5 -179.2 ... 179.2 179.5 179.8\n",
      "  * lat      (lat) float32 0.0 -0.25 -0.5 -0.75 ... -89.25 -89.5 -89.75 -90.0\n",
      "    level    int32 850\n",
      "Data variables:\n",
      "    const    (lat, lon) float64 3.005e-08 9.852e-09 ... 8.685e-09 8.685e-09\n",
      "    spv      (lat, lon) float64 -0.05366 -0.05166 -0.04958 ... 0.07634 0.07634\n",
      "    tw       (lat, lon) float64 -0.387 -0.3898 -0.3934 ... -0.3068 -0.3068\n",
      "    cp       (lat, lon) float64 1.505 1.504 1.503 1.503 ... 0.2285 0.2285 0.2285\n",
      "    ep       (lat, lon) float64 -0.5268 -0.5206 -0.5144 ... 0.0461 0.0461 0.0461\n"
     ]
    }
   ],
   "source": [
    "# U850 - ERA5\n",
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(detrend_timeseries(ua_era5_850_djf.sel(lat=slice(0,-90)).sel(time=slice('1950','2023')).u))  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(detrend_timeseries(stratospheric_polar_vortex_rean[0].sel(time=slice('1950','2023')))),\n",
    "                           'tw': standardize_data(detrend_timeseries(tropical_warming[0].t.sel(time=slice('1950','2023')))),\n",
    "                           'cp': standardize_data(detrend_timeseries(sst_CP_obs[0].sel(time=slice('1950','2023')))),\n",
    "                           'ep': standardize_data(detrend_timeseries(sst_EP_obs[0].sel(time=slice('1950','2023'))))})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'era5')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/without_GMST','u850')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is regressor_coefs: <xarray.Dataset>\n",
      "Dimensions:   (lon: 640, lat: 160, lv_HYBL1: 1)\n",
      "Coordinates:\n",
      "  * lon       (lon) float32 0.0 0.5625 1.125 1.688 ... 357.8 358.3 358.9 359.4\n",
      "  * lat       (lat) float32 -0.2808 -0.8424 -1.404 ... -88.45 -89.01 -89.57\n",
      "  * lv_HYBL1  (lv_HYBL1) int32 12\n",
      "Data variables:\n",
      "    const     (lv_HYBL1, lat, lon) float64 -4.816e-10 2.069e-08 ... -2.95e-10\n",
      "    spv       (lv_HYBL1, lat, lon) float64 -0.2602 -0.2622 ... 0.2657 0.2617\n",
      "    tw        (lv_HYBL1, lat, lon) float64 -0.2484 -0.2413 ... -0.2206 -0.2161\n",
      "    cp        (lv_HYBL1, lat, lon) float64 0.5045 0.5255 ... -0.1084 -0.1172\n",
      "    ep        (lv_HYBL1, lat, lon) float64 -0.5382 -0.544 ... 0.1362 0.1357\n",
      "    gmst      (lv_HYBL1, lat, lon) float64 0.00535 0.001355 ... 0.1795 0.1818\n"
     ]
    }
   ],
   "source": [
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(detrend_timeseries(ua_jra55_850_concat_djf.UGRD_GDS4_HYBL_S123.sel(lat=slice(0,-90)).sel(time=slice('1958','2023'))))  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(detrend_timeseries(stratospheric_polar_vortex_rean[2].UGRD_GDS4_HYBL_S123.sel(time=slice('1958','2023')))), \n",
    "                           'tw': standardize_data(detrend_timeseries(tropical_warming[2].TMP_GDS4_HYBL_S123.sel(time=slice('1958','2023')))), \n",
    "                           'cp': standardize_data(detrend_timeseries(sst_CP_obs[0].sel(time=slice('1958','2023')))), \n",
    "                           'ep': standardize_data(detrend_timeseries(sst_EP_obs[0].sel(time=slice('1958','2023')))),\n",
    "                           'gmst': standardize_data(detrend_timeseries(tas_DJF.sel(time=slice('1958','2023')))),})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep','gmst']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'JRA55')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/JRA55_causal_network/anomalies/detrended_data/with_GMST/','u850')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is regressor_coefs: <xarray.Dataset>\n",
      "Dimensions:  (lat: 37, lon: 144)\n",
      "Coordinates:\n",
      "    level    float32 850.0\n",
      "  * lat      (lat) float32 0.0 -2.5 -5.0 -7.5 -10.0 ... -82.5 -85.0 -87.5 -90.0\n",
      "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "Data variables:\n",
      "    const    (lat, lon) float64 6.751e-09 3.203e-08 ... -3.72e-09 -6.378e-09\n",
      "    spv      (lat, lon) float64 -0.02017 -0.02882 -0.05748 ... 0.04026 0.03192\n",
      "    tw       (lat, lon) float64 -0.3206 -0.3706 -0.4323 ... -0.489 -0.509\n",
      "    cp       (lat, lon) float64 -0.5302 -0.4383 -0.3174 ... -0.1717 -0.1509\n",
      "    ep       (lat, lon) float64 0.6051 0.6061 0.6051 ... 0.5148 0.5131 0.5136\n",
      "    gmst     (lat, lon) float64 0.149 0.07994 -0.02344 ... 0.1369 0.1354 0.1313\n"
     ]
    }
   ],
   "source": [
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(detrend_timeseries(ua_ncep_850_djf.sel(lat=slice(0,-90)).sel(time=slice('1950','2023')).uwnd))  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(detrend_timeseries(stratospheric_polar_vortex_rean[1].uwnd.sel(time=slice('1950','2023')))), \n",
    "                           'tw': standardize_data(detrend_timeseries(tropical_warming[1].air.sel(time=slice('1950','2023')))), \n",
    "                           'cp': standardize_data(detrend_timeseries(sst_CP_obs[0].sel(time=slice('1950','2023')))), \n",
    "                           'ep': standardize_data(detrend_timeseries(sst_EP_obs[0].sel(time=slice('1950','2023')))),\n",
    "                           'gmst': standardize_data(detrend_timeseries(tas_DJF.sel(time=slice('1950','2023')))),})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep','gmst']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'NCEP')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/NCEP_causal_network/anomalies/detrended_data/with_GMST/','u850')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is regressor_coefs: <xarray.Dataset>\n",
      "Dimensions:  (lon: 1440, lat: 361)\n",
      "Coordinates:\n",
      "  * lon      (lon) float32 -180.0 -179.8 -179.5 -179.2 ... 179.2 179.5 179.8\n",
      "  * lat      (lat) float32 0.0 -0.25 -0.5 -0.75 ... -89.25 -89.5 -89.75 -90.0\n",
      "    level    int32 850\n",
      "Data variables:\n",
      "    const    (lat, lon) float64 3.092e-08 1.069e-08 ... 7.131e-09 7.131e-09\n",
      "    spv      (lat, lon) float64 -0.0433 -0.04162 -0.03992 ... 0.05781 0.05781\n",
      "    tw       (lat, lon) float64 -0.3116 -0.3167 -0.3231 ... -0.4418 -0.4418\n",
      "    cp       (lat, lon) float64 1.476 1.475 1.475 1.476 ... 0.282 0.282 0.282\n",
      "    ep       (lat, lon) float64 -0.4963 -0.491 -0.4859 ... -0.008573 -0.008573\n",
      "    gmst     (lat, lon) float64 -0.1098 -0.1064 -0.1024 ... 0.1964 0.1964 0.1964\n"
     ]
    }
   ],
   "source": [
    "# Initialize class\n",
    "mlr = spatial_MLR()\n",
    "\n",
    "# Prepare regression data\n",
    "variable = standardize_data(detrend_timeseries(ua_era5_850_djf.sel(lat=slice(0,-90)).sel(time=slice('1950','2023')).u))  # Your target data array\n",
    "regressors = pd.DataFrame({'spv': standardize_data(detrend_timeseries(stratospheric_polar_vortex_rean[0].sel(time=slice('1950','2023')))), \n",
    "                           'tw': standardize_data(detrend_timeseries(tropical_warming[0].t.sel(time=slice('1950','2023')))), \n",
    "                           'cp': standardize_data(detrend_timeseries(sst_CP_obs[0].sel(time=slice('1950','2023')))), \n",
    "                           'ep': standardize_data(detrend_timeseries(sst_EP_obs[0].sel(time=slice('1950','2023')))),\n",
    "                           'gmst': standardize_data(detrend_timeseries(tas_DJF.sel(time=slice('1950','2023')))),})\n",
    "\n",
    "regressor_names = ['const','spv', 'tw','cp','ep','gmst']  # Regressor names\n",
    "mlr.regression_data(variable, regressors, regressor_names,'era5')\n",
    "\n",
    "# Perform regression\n",
    "mlr.perform_regression('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/','u850')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor_names = ['const','gmst','spv','tw','cp','ep']  # Regressor names\n",
    "maps_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/with_GMST/u850/regression_coefficients_era5.nc')\n",
    "maps_era5_pvals = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/with_GMST/u850/regression_coefficients_pvalues_era5.nc')\n",
    "R2_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/anomalies/detrended_data/with_GMST/u850/R2_era5.nc')\n",
    "maps = [maps_era5[rd] for rd in regressor_names]\n",
    "maps_pvalues = [maps_era5_pvals[rd] for rd in regressor_names]\n",
    "maps[0] = R2_era5.__xarray_dataarray_variable__ * 100\n",
    "titles = ['Fraction of \\n Variance Explained','Global Mean \\n Surface Temperature','Stratospheric \\n Polar Vortex','Tropical Upper Tropospheric \\n Temperature','Central Pacific SST','Eastern Pacific SST']\n",
    "fig = plot_regression_coef_map_MEM(maps, maps_pvalues, titles, '/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/trended_data/with_GMST/u850/ERA5_trend_with_GMST_ua850_coef.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor_names = ['const','gmst','spv','tw','cp','ep']  # Regressor names\n",
    "maps_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/anomalies/detrended_data/with_GMST/u850/regression_coefficients_era5.nc')\n",
    "maps_era5_pvals = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/anomalies/detrended_data/with_GMST/u850/regression_coefficients_pvalues_era5.nc')\n",
    "R2_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/anomalies/detrended_data/with_GMST/u850/R2_era5.nc')\n",
    "maps = [maps_era5[rd] for rd in regressor_names]\n",
    "maps_pvalues = [maps_era5_pvals[rd] for rd in regressor_names]\n",
    "maps[0] = R2_era5.__xarray_dataarray_variable__ * 100\n",
    "titles = ['Fraction of \\n Variance Explained','Global Mean \\n Surface Temperature','Stratospheric \\n Polar Vortex','Tropical Upper Tropospheric \\n Temperature','Central Pacific SST','Eastern Pacific SST']\n",
    "fig = plot_regression_coef_map_row(maps, maps_pvalues, titles, '/home/jmindlin/causal_EDJ/ERA5_causal_network/anomalies/detrended_data/with_GMST/u850/ERA5_detrend_with_GMST_ua850_coef_row.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor_names = ['const','gmst','spv','tw','cp','ep']  # Regressor names\n",
    "maps_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/u850/regression_coefficients_era5.nc')\n",
    "maps_era5_pvals = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/u850/regression_coefficients_pvalues_era5.nc')\n",
    "R2_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/u850/R2_era5.nc')\n",
    "maps = [maps_era5[rd] for rd in regressor_names]\n",
    "maps_pvalues = [maps_era5_pvals[rd] for rd in regressor_names]\n",
    "maps[0] = R2_era5.__xarray_dataarray_variable__ * 100\n",
    "titles = ['Fraction of \\n Variance Explained','Global Mean \\n Surface Temperature','Stratospheric \\n Polar Vortex','Tropical Upper Tropospheric \\n Temperature','Central Pacific SST','Eastern Pacific SST']\n",
    "fig = plot_regression_coef_map_MEM(maps, maps_pvalues, titles, '/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/u850/ERA5_detrend_with_GMST_ua850_coef.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_names = ['const','gmst','spv','tw','cp','ep']  # Regressor names\n",
    "maps_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/u850/regression_coefficients_era5.nc')\n",
    "maps_era5_pvals = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/u850/regression_coefficients_pvalues_era5.nc')\n",
    "R2_era5 = xr.open_dataset('/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/u850/R2_era5.nc')\n",
    "maps = [maps_era5[rd] for rd in regressor_names]\n",
    "maps_pvalues = [maps_era5_pvals[rd] for rd in regressor_names]\n",
    "maps[0] = R2_era5.__xarray_dataarray_variable__ * 100\n",
    "titles = ['Fraction of \\n Variance Explained','Global Mean \\n Surface Temperature','Stratospheric \\n Polar Vortex','Tropical Upper Tropospheric \\n Temperature','Central Pacific SST','Eastern Pacific SST']\n",
    "fig = plot_regression_coef_map_row(maps, maps_pvalues, titles, '/home/jmindlin/causal_EDJ/ERA5_causal_network/raw_data/detrended_data/with_GMST/u850/ERA5_detrend_with_GMST_ua850_coef_row.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmval_julia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
